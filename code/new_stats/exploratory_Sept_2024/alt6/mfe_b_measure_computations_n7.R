# This script will load, and organize the pavlovia data. Then, computes measures of interest.
# For each participant, a single, new, organized csv file that has all the necessary information will be generated.
# Author: Kianoosh Hosseini at NDCLab @FIU (https://Kianoosh.info; https://NDClab.com)
# Last Update: 2024-09-05 (YYYY-MM-DD)

# This version computes average confidence ratings.

library(tidyverse)
library(dplyr)
library(stringr)


#Working directory should be the Psychopy experiment directory.
proje_wd <- "/Users/kihossei/Documents/GitHub/memory-for-error-dataset"
setwd(proje_wd)

input_raw_path <- paste(proje_wd, "sourcedata", "raw","psychopy", sep ="/", collapse = NULL) # input data directory
input_organized_path <- paste(proje_wd, "derivatives", "preprocessed", "psychopy", "organizer_output", sep ="/", collapse = NULL) # input directory for data files generated by the organizer script!
output_path <- paste(proje_wd, "derivatives", "preprocessed", "psychopy", "stat_output", sep ="/", collapse = NULL) # output directory
proc_fileName <- "processed_data_mfe_b_Proj_n7.csv" # output filename
flanker_csv_fileName <- "_mfe_b_flankerDat_n1.csv" # each output csv file will have this on its filename
surprise_csv_fileName <- "_mfe_b_surpriseDat_n1.csv" # each output csv file will have this on its filename


## creating a list of all raw data csv files in the input folder.
raw_datafiles_list <- c() # an empty list that will be filled in the next "for" loop!
csvSelect <- list.files(input_raw_path, pattern = ".csv") # listing only csv files
for (i in 1:length(csvSelect)){
  temp_for_file <- ifelse (str_detect(csvSelect[i], "mfe_b", negate = FALSE), 1, 0)
  if (temp_for_file == 1){
    temp_list <- csvSelect[i]
    raw_datafiles_list <- c(raw_datafiles_list, temp_list)
  }
}

# Creating the main empty dataframe that will be filled with the data from the loop below:
main_df <- setNames(data.frame(matrix(ncol = 37, nrow = 0)), c("participant_id", "congAcc", "incongAcc",
                                                               "incongruent_dat_meanRT", "errorDat_meanRT", "congruent_dat_meanRT", "corrDat_meanRT",
                                                               "congCorr_meanRT", "incongCorr_meanRT", "congCorr_logMeanRT",
                                                               "congErr_meanRT", "incongErr_meanRT", "congErr_logMeanRT", "incongErr_logMeanRT",
                                                               "incongCorr_logMeanRT", "flankEff_meanACC", "flankEff_meanRT", "flankEff_logMeanRT",
                                                               "reported_errors", "committed_errors", "memoryBias_score", "number_of_surp_resp_items_used",
                                                               "overall_hitRate", "overall_false_alarm_rate",
                                                               "num_removed_fast_trials_in_memory_surp", "avg_confidence_allFaces",
                                                               "avg_confidence_hitFaces", "avg_confidence_missFaces", "avg_confidence_FA_Faces",
                                                               "avg_confidence_CR_Faces", "avg_confidence_error_hitFaces", "avg_confidence_error_missFaces",
                                                               "avg_confidence_correct_hitFaces", "avg_confidence_correct_missFaces", "avg_confidence_diff_allFaces", "avg_confidence_error_allFaces", "avg_confidence_correct_allFaces"))

# Counters for the number of excluded people based on each criterion
num_of_participants_removed_based_on_memory_surp_trial_removal <- 0 # Will be updated in the loop below
num_of_participants_removed_based_on_accuracy <- 0 # Will be updated in the loop below
num_of_participants_removed_based_on_incong_error_num <- 0 # Will be updated in the loop below
num_participants_removed_based_on_num_incong_error_faces_in_memory_surp <- 0 # Will be updated in the loop below
num_of_participants_removed_based_on_surp_response_frequency_andOR_repeated_ans <- 0 # Will be updated in the loop below
num_of_outlier_subs_based_on_sample_rt_in_surp <- 0
num_of_participants_removed_based_on_least_confidency <- 0

# Looping over all participants
for (subject in 1:length(raw_datafiles_list)){
  # for this participant, find the raw csv file
  psychopy_file <- paste(input_raw_path,raw_datafiles_list[subject], sep = "/", collapse = NULL)

  #read in the data for this participant, establish id, and remove extraneous variables
  psychopyDat <- read.csv(file = psychopy_file, stringsAsFactors = FALSE, na.strings=c("", "NA"))
  participant_id <- psychopyDat$id[1]

  # Load this participant's flanker and surprise data frames
  flanker_name <- paste0(participant_id, flanker_csv_fileName, sep = "", collapse = NULL)
  surprise_name <- paste0(participant_id, surprise_csv_fileName, sep = "", collapse = NULL)
  flanker_df <- read.csv(file = paste(input_organized_path, flanker_name, sep = "/", collapse = NULL), stringsAsFactors = FALSE, na.strings=c("", "NA"))
  surprise_df <- read.csv(file = paste(input_organized_path, surprise_name, sep = "/", collapse = NULL), stringsAsFactors = FALSE, na.strings=c("", "NA"))


  # Check to see if this participant has the flanker accuracy above 60%
  if (mean(as.numeric(flanker_df$current_trial_accuracy)) >= 0.6){


    # Removing participants based on whether they just pressed the keys without actually paying attention to the task or they were too slow in responding.
    # We check this by "keep_surp_memory_trial_based_on_subjectLevel_rt" variables
    # in the surprise_df data frame. O means that surprise trial needs to be removed.
    # If more than 37.5% of surprise trials are removed, we exclude that participant.
    num_removed_fast_trials_in_memory_surp <- as.numeric(surprise_df$num_tooFast_surpTrials[1])
    number_of_faces_in_surp_memory_task <- nrow(surprise_df)
    rt_exclusion_threshold <- round(0.375 * number_of_faces_in_surp_memory_task)

    if (num_removed_fast_trials_in_memory_surp <= rt_exclusion_threshold){ # Participants who have less than rt_exclusion_threshold surprise
      # trials removed, will be included.

      # If this participant has at least 3 blocks marked for exclusion (based on response frequency and repeated responses), we exclude them.
      if (surprise_df$num_blocks_excluded[1] <= 3){

        # Check how confident participants were in their responses during the surprise task.
        # If a participant was least confident in 60 % of surprise trials, we exclude that subject.
        confidency_counts <- table(surprise_df$how_confident)
        if (confidency_counts[[1]] < 346 ){

          # check to see if the participant has at least 8 legit incongruent errors or not
          incong_flankerDat <- filter(flanker_df, current_trial_congruency == 0 ) # selecting only incongruent trials
          cong_flankerDat <- filter(flanker_df, current_trial_congruency == 1) # selecting only congruent trials
          error_incong_flankerDat <- filter(incong_flankerDat, current_trial_accuracy == 0)
          legit_error_incong_flankerDat <- filter(error_incong_flankerDat, current_trial_legitResponse == 1)
          if (nrow(legit_error_incong_flankerDat) >= 8 ){

            # Checking to see if there are at least 8 incongruent error faces in the surprise_df of this participant.
            # we need to remove the trials that are marked based on rt!
            surprise_df <- filter(surprise_df, keep_surp_memory_trial_based_on_subjectLevel_rt == 1)

            num_incong_error_faces_in_surp <- 0
            # Counting the number of legit incong error faces available in surprise_df!
            for (rr in 1:nrow(legit_error_incong_flankerDat)){
              temp_face <- legit_error_incong_flankerDat$current_trial_face[rr]
              temp_for_surp <- filter(surprise_df, face == temp_face)
              errorFace_exist_in_surpDat <- ifelse(nrow(temp_for_surp) == 1, 1,0)
              if (errorFace_exist_in_surpDat == 1){
                num_incong_error_faces_in_surp <- num_incong_error_faces_in_surp + 1
              }
            } # Closing the loop that counts the number of incong error faces available in surprise_df!
            if (num_incong_error_faces_in_surp >= 8){

              incongAcc <- mean(as.numeric(incong_flankerDat$current_trial_accuracy))
              congAcc <- mean(as.numeric(cong_flankerDat$current_trial_accuracy))

              # subset the data for correct and error trials, separately for congruent and incongruent trials, creating new data frames for each
              corrDat <- flanker_df[flanker_df$current_trial_accuracy == 1,]
              corrDat_meanRT <- mean(corrDat$current_trial_rt, na.rm = TRUE)

              congruent_dat <- flanker_df[flanker_df$current_trial_congruency == 1,]
              congruent_dat_meanRT <- mean(congruent_dat$current_trial_rt, na.rm = TRUE)

              cong_corrDat <- corrDat[corrDat$current_trial_congruency == 1,]
              incong_corrDat <- corrDat[corrDat$current_trial_congruency == 0,]

              errorDat <- flanker_df[flanker_df$current_trial_accuracy == 0,]
              errorDat_meanRT <- mean(errorDat$current_trial_rt, na.rm = TRUE)

              incongruent_dat <- flanker_df[flanker_df$current_trial_congruency == 0,]
              incongruent_dat_meanRT <- mean(incongruent_dat$current_trial_rt, na.rm = TRUE)


              cong_errorDat <- errorDat[errorDat$current_trial_congruency == 1,]
              incong_errorDat <- errorDat[errorDat$current_trial_congruency == 0,]
              #for correct trials, compute mean RT (raw and log-corrected)
              congCorr_meanRT <- mean(cong_corrDat$current_trial_rt, na.rm = TRUE)
              incongCorr_meanRT <- mean(incong_corrDat$current_trial_rt, na.rm = TRUE)

              congErr_meanRT <- mean(cong_errorDat$current_trial_rt, na.rm = TRUE)
              incongErr_meanRT <- mean(incong_errorDat$current_trial_rt, na.rm = TRUE)

              congCorr_logMeanRT <- mean(log((1+cong_corrDat$current_trial_rt)), na.rm = TRUE)
              incongCorr_logMeanRT <- mean(log((1+incong_corrDat$current_trial_rt)), na.rm = TRUE)

              congErr_logMeanRT <- mean(log((1+cong_errorDat$current_trial_rt)), na.rm = TRUE)
              incongErr_logMeanRT <- mean(log((1+incong_errorDat$current_trial_rt)), na.rm = TRUE)

              # compute flanker-effect scores for accuracy, RT, log-RT
              flankEff_meanACC <- incongAcc - congAcc
              flankEff_meanRT <- incongCorr_meanRT - congCorr_meanRT
              flankEff_logMeanRT <- incongCorr_logMeanRT - congCorr_logMeanRT

              num_tooFast_surpTrials  <- surprise_df$num_tooFast_surpTrials[1]

              # number of committed errors in the flanker task
              committed_errors <- nrow(errorDat)
              # number of reported errors
              psychopyDatTrim <- psychopyDat[("textbox_errorNum.text")] # stores the number of reported errors by subjects
              reported_errors <- subset(psychopyDatTrim, complete.cases(psychopyDatTrim$textbox_errorNum.text))
              reported_errors <- reported_errors$textbox_errorNum.text # number of reported errors by participants
              reported_errors <- str_extract_all(reported_errors, '\\d+\\.?\\d*') # to extract all sequences of digits from the input string
              # There was a participant who had reported 70/100. values 70 and 100. To solve this issue, I replace this kind of values with NAs!
              if (length(reported_errors) == 0){ # there is no reported errors
                reported_errors <- NA
                memoryBias_score <- NA
              } else {
                reported_errors <- parse_number(reported_errors[[1]]) # in cases like 70/100, we will have 70 100 at this stage. So, length(reported_errors) will
                # be higher than 1.
                if (length(reported_errors) > 1){ # In case they have reported sth like 70/100
                  reported_errors <- NA
                  memoryBias_score <- NA
                } else if (length(reported_errors) == 1){
                  reported_errors <- reported_errors[1]
                  memoryBias_score <- ((reported_errors - committed_errors)/ reported_errors) # percent bias score calculation
                }
              }
              # The number of response items (out of 6) that the current partcipant has used in the surprise task
              number_of_surp_resp_items_used <- surprise_df$number_of_surp_resp_items_used[1]

              ##########################################################################################################
              ################################### COMPUTE AVERAGE Confidence ratings  ##################################
              ##########################################################################################################
              # The average confidence rating for all responses in the surprise_df
              num_allFaces_reported_def <- 0
              num_allFaces_reported_prob <- 0
              num_allFaces_reported_may <- 0

              for (iii in 1:nrow(surprise_df)){
                if (surprise_df$how_confident[iii] == 1){
                  num_allFaces_reported_may <- num_allFaces_reported_may + 1
                } else if (surprise_df$how_confident[iii] == 3){
                  num_allFaces_reported_prob <- num_allFaces_reported_prob + 1
                } else if (surprise_df$how_confident[iii] == 5){
                  num_allFaces_reported_def <- num_allFaces_reported_def + 1
                }
              }

              avg_confidence_allFaces <- ((num_allFaces_reported_may * 1) + (num_allFaces_reported_prob * 3) + (num_allFaces_reported_def * 5)) / (num_allFaces_reported_may + num_allFaces_reported_prob + num_allFaces_reported_def)

              # The average confidence rating for hit responses (identifying old images as old) in the surprise_df.
              oldFaces_in_surprise_df <- filter(surprise_df, is_new == 0)
              oldFaces_identified_as_old_in_surprise_df <- filter(oldFaces_in_surprise_df, identified_as_new == 0)

              num_hitFaces_reported_def <- 0
              num_hitFaces_reported_prob <- 0
              num_hitFaces_reported_may <- 0

              for (iii in 1:nrow(oldFaces_identified_as_old_in_surprise_df)){
                if (oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 1){
                  num_hitFaces_reported_may <- num_hitFaces_reported_may + 1
                } else if (oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 3){
                  num_hitFaces_reported_prob <- num_hitFaces_reported_prob + 1
                } else if (oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 5){
                  num_hitFaces_reported_def <- num_hitFaces_reported_def + 1
                }
              }
              avg_confidence_hitFaces <- ((num_hitFaces_reported_may * 1) + (num_hitFaces_reported_prob * 3) + (num_hitFaces_reported_def * 5)) / (num_hitFaces_reported_may + num_hitFaces_reported_prob + num_hitFaces_reported_def)

              # The average confidence rating for miss responses (identifying old images as new) in the surprise_df.
              oldFaces_identified_as_new_in_surprise_df <- filter(oldFaces_in_surprise_df, identified_as_new == 1)

              num_missFaces_reported_def <- 0
              num_missFaces_reported_prob <- 0
              num_missFaces_reported_may <- 0

              for (iii in 1:nrow(oldFaces_identified_as_new_in_surprise_df)){
                if (oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 1){
                  num_missFaces_reported_may <- num_missFaces_reported_may + 1
                } else if (oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 3){
                  num_missFaces_reported_prob <- num_missFaces_reported_prob + 1
                } else if (oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 5){
                  num_missFaces_reported_def <- num_missFaces_reported_def + 1
                }
              }
              avg_confidence_missFaces <- ((num_missFaces_reported_may * 1) + (num_missFaces_reported_prob * 3) + (num_missFaces_reported_def * 5)) / (num_missFaces_reported_may + num_missFaces_reported_prob + num_missFaces_reported_def)

              # The average confidence rating for false alarm responses (identifying new images as old) in the surprise_df.
              newFaces_in_surprise_df <- filter(surprise_df, is_new == 1)
              newFaces_identified_as_old_in_surprise_df <- filter(newFaces_in_surprise_df, identified_as_new == 0)

              num_FA_Faces_reported_def <- 0
              num_FA_Faces_reported_prob <- 0
              num_FA_Faces_reported_may <- 0

              for (iii in 1:nrow(newFaces_identified_as_old_in_surprise_df)){
                if (newFaces_identified_as_old_in_surprise_df$how_confident[iii] == 1){
                  num_FA_Faces_reported_may <- num_FA_Faces_reported_may + 1
                } else if (newFaces_identified_as_old_in_surprise_df$how_confident[iii] == 3){
                  num_FA_Faces_reported_prob <- num_FA_Faces_reported_prob + 1
                } else if (newFaces_identified_as_old_in_surprise_df$how_confident[iii] == 5){
                  num_FA_Faces_reported_def <- num_FA_Faces_reported_def + 1
                }
              }
              avg_confidence_FA_Faces <- ((num_FA_Faces_reported_may * 1) + (num_FA_Faces_reported_prob * 3) + (num_FA_Faces_reported_def * 5)) / (num_FA_Faces_reported_may + num_FA_Faces_reported_prob + num_FA_Faces_reported_def)

              # The average confidence rating for correct rejection responses (identifying new images as new) in the surprise_df.
              newFaces_identified_as_new_in_surprise_df <- filter(newFaces_in_surprise_df, identified_as_new == 1)

              num_CR_Faces_reported_def <- 0
              num_CR_Faces_reported_prob <- 0
              num_CR_Faces_reported_may <- 0

              for (iii in 1:nrow(newFaces_identified_as_new_in_surprise_df)){
                if (newFaces_identified_as_new_in_surprise_df$how_confident[iii] == 1){
                  num_CR_Faces_reported_may <- num_CR_Faces_reported_may + 1
                } else if (newFaces_identified_as_new_in_surprise_df$how_confident[iii] == 3){
                  num_CR_Faces_reported_prob <- num_CR_Faces_reported_prob + 1
                } else if (newFaces_identified_as_new_in_surprise_df$how_confident[iii] == 5){
                  num_CR_Faces_reported_def <- num_CR_Faces_reported_def + 1
                }
              }
              avg_confidence_CR_Faces <- ((num_CR_Faces_reported_may * 1) + (num_CR_Faces_reported_prob * 3) + (num_CR_Faces_reported_def * 5)) / (num_CR_Faces_reported_may + num_CR_Faces_reported_prob + num_CR_Faces_reported_def)



              ########################### Average confidence ratings for only error faces in the surprise_df
              error_surprise_df <- data.frame()
              for (iii in 1:nrow(legit_error_incong_flankerDat)){
                temp_face_from_flanker <- legit_error_incong_flankerDat$current_trial_face[iii]
                temp_face_row_in_surp <- filter(surprise_df, face == temp_face_from_flanker)
                if (nrow(temp_face_row_in_surp) == 1){ # if old incong error face exist in the surprise_df
                  error_surprise_df <- rbind(error_surprise_df, temp_face_row_in_surp)
                }
              }
              # The average confidence rating for all error responses in the surprise_df using 6 ratings.

              num_error_allFaces_reported_def_old <- 0
              num_error_allFaces_reported_prob_old <- 0
              num_error_allFaces_reported_may_old <- 0
              num_error_allFaces_reported_def_new <- 0
              num_error_allFaces_reported_prob_new <- 0
              num_error_allFaces_reported_may_new <- 0

              for (iii in 1:nrow(error_surprise_df)){
                if (error_surprise_df$how_confident_all[iii] == 1){
                  num_error_allFaces_reported_may_old <- num_error_allFaces_reported_may_old + 1
                } else if (error_surprise_df$how_confident_all[iii] == 3){
                  num_error_allFaces_reported_prob_old <- num_error_allFaces_reported_prob_old + 1
                } else if (error_surprise_df$how_confident_all[iii] == 5){
                  num_error_allFaces_reported_def_old <- num_error_allFaces_reported_def_old + 1
                } else if (error_surprise_df$how_confident_all[iii] == -1){ # the face is identified as maybe new
                  num_error_allFaces_reported_may_new <- num_error_allFaces_reported_may_new + 1
                } else if (error_surprise_df$how_confident_all[iii] == -3){ # the face is identified as probably new
                  num_error_allFaces_reported_prob_new <- num_error_allFaces_reported_prob_new + 1
                } else if (error_surprise_df$how_confident_all[iii] == -5){ # the face is identified as definitely new
                  num_error_allFaces_reported_def_new <- num_error_allFaces_reported_def_new + 1
                }
              }
              avg_confidence_error_allFaces <- ((num_error_allFaces_reported_may_old * 1) + (num_error_allFaces_reported_prob_old * 3) + (num_error_allFaces_reported_def_old * 5)
              + (num_error_allFaces_reported_may_new * (-1)) + (num_error_allFaces_reported_prob_new * (-3)) + (num_error_allFaces_reported_def_new * (-5))) / (num_error_allFaces_reported_def_old + num_error_allFaces_reported_prob_old + num_error_allFaces_reported_may_old + num_error_allFaces_reported_def_new + num_error_allFaces_reported_prob_new + num_error_allFaces_reported_may_new)



              # The average confidence rating for hit error responses (identifying error old images as old) in the surprise_df.
              error_oldFaces_identified_as_old_in_surprise_df <- filter(error_surprise_df, identified_as_new == 0)

              num_error_hitFaces_reported_def <- 0
              num_error_hitFaces_reported_prob <- 0
              num_error_hitFaces_reported_may <- 0

              for (iii in 1:nrow(error_oldFaces_identified_as_old_in_surprise_df)){
                if (error_oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 1){
                  num_error_hitFaces_reported_may <- num_error_hitFaces_reported_may + 1
                } else if (error_oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 3){
                  num_error_hitFaces_reported_prob <- num_error_hitFaces_reported_prob + 1
                } else if (error_oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 5){
                  num_error_hitFaces_reported_def <- num_error_hitFaces_reported_def + 1
                }
              }
              avg_confidence_error_hitFaces <- ((num_error_hitFaces_reported_may * 1) + (num_error_hitFaces_reported_prob * 3) + (num_error_hitFaces_reported_def * 5)) / (num_error_hitFaces_reported_may + num_error_hitFaces_reported_prob + num_error_hitFaces_reported_def)

              # The average confidence rating for miss responses (identifying old images as new) in the surprise_df.
              error_oldFaces_identified_as_new_in_surprise_df <- filter(error_surprise_df, identified_as_new == 1)

              num_error_missFaces_reported_def <- 0
              num_error_missFaces_reported_prob <- 0
              num_error_missFaces_reported_may <- 0

              for (iii in 1:nrow(error_oldFaces_identified_as_new_in_surprise_df)){
                if (error_oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 1){
                  num_error_missFaces_reported_may <- num_error_missFaces_reported_may + 1
                } else if (error_oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 3){
                  num_error_missFaces_reported_prob <- num_error_missFaces_reported_prob + 1
                } else if (error_oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 5){
                  num_error_missFaces_reported_def <- num_error_missFaces_reported_def + 1
                }
              }
              avg_confidence_error_missFaces <- ((num_error_missFaces_reported_may * 1) + (num_error_missFaces_reported_prob * 3) + (num_error_missFaces_reported_def * 5)) / (num_error_missFaces_reported_may + num_error_missFaces_reported_prob + num_error_missFaces_reported_def)


              ########################### Average confidence ratings for only correct faces in the surprise_df
              legit_correct_incong_flankerDat <- filter(incong_corrDat, current_trial_legitResponse == 1)
              correct_surprise_df <- data.frame()
              for (iii in 1:nrow(legit_correct_incong_flankerDat)){
                temp_face_from_flanker <- legit_correct_incong_flankerDat$current_trial_face[iii]
                temp_face_row_in_surp <- filter(surprise_df, face == temp_face_from_flanker)
                if (nrow(temp_face_row_in_surp) == 1){ # if old incong error face exist in the surprise_df
                  correct_surprise_df <- rbind(correct_surprise_df, temp_face_row_in_surp)
                }
              }

              # The average confidence rating for all correct responses in the surprise_df.

              num_correct_allFaces_reported_def_old <- 0
              num_correct_allFaces_reported_prob_old <- 0
              num_correct_allFaces_reported_may_old <- 0
              num_correct_allFaces_reported_def_new <- 0
              num_correct_allFaces_reported_prob_new <- 0
              num_correct_allFaces_reported_may_new <- 0

              for (iii in 1:nrow(correct_surprise_df)){
                if (correct_surprise_df$how_confident_all[iii] == 1){
                  num_correct_allFaces_reported_may_old <- num_correct_allFaces_reported_may_old + 1
                } else if (correct_surprise_df$how_confident_all[iii] == 3){
                  num_correct_allFaces_reported_prob_old <- num_correct_allFaces_reported_prob_old + 1
                } else if (correct_surprise_df$how_confident_all[iii] == 5){
                  num_correct_allFaces_reported_def_old <- num_correct_allFaces_reported_def_old + 1
                } else if (correct_surprise_df$how_confident_all[iii] == -1){ # the face is identified as maybe new
                  num_correct_allFaces_reported_may_new <- num_correct_allFaces_reported_may_new + 1
                } else if (correct_surprise_df$how_confident_all[iii] == -3){ # the face is identified as probably new
                  num_correct_allFaces_reported_prob_new <- num_correct_allFaces_reported_prob_new + 1
                } else if (correct_surprise_df$how_confident_all[iii] == -5){ # the face is identified as definitely new
                  num_correct_allFaces_reported_def_new <- num_correct_allFaces_reported_def_new + 1
                }
              }
              avg_confidence_correct_allFaces <- ((num_correct_allFaces_reported_may_old * 1) + (num_correct_allFaces_reported_prob_old * 3) + (num_correct_allFaces_reported_def_old * 5)
                + (num_correct_allFaces_reported_may_new * (-1)) + (num_correct_allFaces_reported_prob_new * (-3)) + (num_correct_allFaces_reported_def_new * (-5))) / (num_correct_allFaces_reported_def_old + num_correct_allFaces_reported_prob_old + num_correct_allFaces_reported_may_old + num_correct_allFaces_reported_def_new + num_correct_allFaces_reported_prob_new + num_correct_allFaces_reported_may_new)


              # The average confidence rating for hit correct responses (identifying correct old images as old) in the surprise_df.
              correct_oldFaces_identified_as_old_in_surprise_df <- filter(correct_surprise_df, identified_as_new == 0)

              num_correct_hitFaces_reported_def <- 0
              num_correct_hitFaces_reported_prob <- 0
              num_correct_hitFaces_reported_may <- 0

              for (iii in 1:nrow(correct_oldFaces_identified_as_old_in_surprise_df)){
                if (correct_oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 1){
                  num_correct_hitFaces_reported_may <- num_correct_hitFaces_reported_may + 1
                } else if (correct_oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 3){
                  num_correct_hitFaces_reported_prob <- num_correct_hitFaces_reported_prob + 1
                } else if (correct_oldFaces_identified_as_old_in_surprise_df$how_confident[iii] == 5){
                  num_correct_hitFaces_reported_def <- num_correct_hitFaces_reported_def + 1
                }
              }
              avg_confidence_correct_hitFaces <- ((num_correct_hitFaces_reported_may * 1) + (num_correct_hitFaces_reported_prob * 3) + (num_correct_hitFaces_reported_def * 5)) / (num_correct_hitFaces_reported_may + num_correct_hitFaces_reported_prob + num_correct_hitFaces_reported_def)

              # The average confidence rating for miss responses (identifying old images as new) in the surprise_df.
              correct_oldFaces_identified_as_new_in_surprise_df <- filter(correct_surprise_df, identified_as_new == 1)

              num_correct_missFaces_reported_def <- 0
              num_correct_missFaces_reported_prob <- 0
              num_correct_missFaces_reported_may <- 0

              for (iii in 1:nrow(correct_oldFaces_identified_as_new_in_surprise_df)){
                if (correct_oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 1){
                  num_correct_missFaces_reported_may <- num_correct_missFaces_reported_may + 1
                } else if (correct_oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 3){
                  num_correct_missFaces_reported_prob <- num_correct_missFaces_reported_prob + 1
                } else if (correct_oldFaces_identified_as_new_in_surprise_df$how_confident[iii] == 5){
                  num_correct_missFaces_reported_def <- num_correct_missFaces_reported_def + 1
                }
              }
              avg_confidence_correct_missFaces <- ((num_correct_missFaces_reported_may * 1) + (num_correct_missFaces_reported_prob * 3) + (num_correct_missFaces_reported_def * 5)) / (num_correct_missFaces_reported_may + num_correct_missFaces_reported_prob + num_correct_missFaces_reported_def)


              # Difference score (error - correct) for average confidence ratings

              avg_confidence_diff_allFaces <- avg_confidence_error_allFaces - avg_confidence_correct_allFaces
              ##########################################################################################################
              ##########################################################################################################
              ##########################################################################################################



              # Overall hits, misses, FAs, and Correct rejections regardless of errors and corrects.
              hit_num <- 0
              miss_num <- 0
              corr_rej_num <- 0 # Correct rejections number is the same for Errors, corrects as it is overall. It means they identify new faces as new!
              false_alrams_num <- 0

              for (surpTrial in 1:nrow(surprise_df)){
                if (surprise_df$is_new[surpTrial] == 0){ # The face is old
                  if (surprise_df$identified_as_new[surpTrial] == 0) { # The old face is identified correctly as old
                    hit_num <- hit_num + 1
                  } else if (surprise_df$identified_as_new[surpTrial] == 1){ # The old face is identified incorrectly as new
                    miss_num <- miss_num + 1
                  }
                } else if (surprise_df$is_new[surpTrial] == 1){ # The face is new
                  if (surprise_df$identified_as_new[surpTrial] == 0) { # The new face is identified incorrectly as old
                    false_alrams_num <- false_alrams_num + 1
                  } else if (surprise_df$identified_as_new[surpTrial] == 1){ # The new face is identified correctly as new
                    corr_rej_num <- corr_rej_num + 1
                  }
                }
              } # Closing the loop over surprise_df trials

              overall_hitRate <- (hit_num) / ((hit_num) + miss_num) # hit rate
              overall_false_alarm_rate <- (false_alrams_num)/ ((false_alrams_num) + (corr_rej_num))

              #### filling the main data frame
              main_df[nrow(main_df) + 1,] <-c(participant_id, congAcc, incongAcc,
                                              incongruent_dat_meanRT, errorDat_meanRT, congruent_dat_meanRT, corrDat_meanRT,
                                              congCorr_meanRT, incongCorr_meanRT, congCorr_logMeanRT,
                                              congErr_meanRT, incongErr_meanRT, congErr_logMeanRT, incongErr_logMeanRT,
                                              incongCorr_logMeanRT, flankEff_meanACC, flankEff_meanRT, flankEff_logMeanRT,
                                              reported_errors, committed_errors, memoryBias_score, number_of_surp_resp_items_used,
                                              overall_hitRate, overall_false_alarm_rate,
                                              num_removed_fast_trials_in_memory_surp, avg_confidence_allFaces,
                                              avg_confidence_hitFaces, avg_confidence_missFaces, avg_confidence_FA_Faces,
                                              avg_confidence_CR_Faces, avg_confidence_error_hitFaces, avg_confidence_error_missFaces,
                                              avg_confidence_correct_hitFaces, avg_confidence_correct_missFaces,
                                              avg_confidence_diff_allFaces, avg_confidence_error_allFaces, avg_confidence_correct_allFaces)


            } else { # If a participant has been excluded because they had less than 8 legit incong error faces in the surprise memory task, we add 1 to the counter below.
              num_participants_removed_based_on_num_incong_error_faces_in_memory_surp <- num_participants_removed_based_on_num_incong_error_faces_in_memory_surp + 1
              ### Printing output
              print(paste("Participant ", participant_id, " was excluded because they had less than 8 legit incong error faces in the surprise memory task."))
              #### end of printing output
            }
          } else { # If a participant has been excluded because they had less than 8 legit incong errors in the flanker task, we add 1 to the counter below.
            num_of_participants_removed_based_on_incong_error_num <- num_of_participants_removed_based_on_incong_error_num + 1
            ### Printing output
            print(paste("Participant ", participant_id, " was excluded due to having less than 8 legit incongruent errors."))
            #### end of printing output
          }
        } else { # If a participant was least confident in 60 % of surprise trials,we exclude them
          num_of_participants_removed_based_on_least_confidency <- num_of_participants_removed_based_on_least_confidency + 1
          ### Printing output
          print(paste("Participant ", participant_id, " was excluded due to least confidency criterion. Their least confidence percentage was ", (confidency_counts[[1]]/576)*100))
          #### end of printing output
        }
      } else { # if a participant has been excluded based on whether they have chosen the most chosen surprise response >= 90% and/or repeatedly choosing the same answer (12 times in a row at least)!
        num_of_participants_removed_based_on_surp_response_frequency_andOR_repeated_ans <- num_of_participants_removed_based_on_surp_response_frequency_andOR_repeated_ans + 1
        ### Printing output
        print(paste("Participant ", participant_id, " was excluded due to response frequency and/or repeated answer criteria."))
        #### end of printing output
      }
    } else { # If a participant has been excluded because they had more than 37.5% surp trial removed, we add 1 to the counter below.
      num_of_participants_removed_based_on_memory_surp_trial_removal <- num_of_participants_removed_based_on_memory_surp_trial_removal + 1
      ### Printing output
      print(paste("Participant ", participant_id, " was excluded due to having more 25% surprise trials removed."))
      #### end of printing output
    }
  } else { # If a participant has been excluded because they had less than 60% flanker accuracy, we add 1 to the counter below.
    num_of_participants_removed_based_on_accuracy <- num_of_participants_removed_based_on_accuracy + 1
    ### Printing output
    print(paste("Participant ", participant_id, " was excluded due to having flanker accuracy less than 60%."))
    #### end of printing output
  }
} # Closing the loop for each participant



### Loading RedCap questionnaire data
redcapDat <- read.csv(file = "/Users/kihossei/Documents/GitHub/memory-for-error-dataset/derivatives/preprocessed/redCap/202205v0memoryforerr_SCRD_2023-07-17_0911.csv")
redcapDat_sfe <- read.csv(file = "/Users/kihossei/Documents/GitHub/memory-for-error-dataset/derivatives/preprocessed/redCap/202203v0socialflanke_SCRD_2023-08-22_1524.csv")

# stai state before and after the flanker task were not properly scored on the HPC due to an error in the naming.
# They will be scored here.
# STAI state before
redcapDat$stai5_scrdS_s1_r1_e1 <- rowSums(redcapDat[ , c("stai5_i1_s1_r1_e1", "stai5_i2_s1_r1_e1", "stai5_i3_s1_r1_e1", "stai5_i4_s1_r1_e1","stai5_i5_s1_r1_e1")])
redcapDat_sfe$stai5_scrdS_s1_r1_e1 <- rowSums(redcapDat_sfe[ , c("stai5_i1_s1_r1_e1", "stai5_i2_s1_r1_e1", "stai5_i3_s1_r1_e1", "stai5_i4_s1_r1_e1", "stai5_i5_s1_r1_e1")])
# STAI state After
redcapDat$stai5_scrdS_s1_r1_e2 <- rowSums(redcapDat[ , c("stai5_i1_s1_r1_e1_v2","stai5_i2_s1_r1_e1_v2", "stai5_i3_s1_r1_e1_v2", "stai5_i4_s1_r1_e1_v2","stai5_i5_s1_r1_e1_v2")])
# STAI state difference (After minus Before)
redcapDat$stai5_scrdS_diff <- (redcapDat$stai5_scrdS_s1_r1_e2 - redcapDat$stai5_scrdS_s1_r1_e1)

# scoring postTask_D questionnaire (sum posttaskd_i1, posttaskd_i3, posttaskd_i4,posttaskd_i5)
redcapDat$postTask_d_s1_r1_e1 <- rowSums(redcapDat[ , c("posttaskd_i1_s1_r1_e1","posttaskd_i3_s1_r1_e1", "posttaskd_i4_s1_r1_e1", "posttaskd_i5_s1_r1_e1")])

# Keeping the columns that we need!
redcapDat <- redcapDat[c("record_id", "scaared_b_scrdSoc_s1_r1_e1", "scaared_b_scrdTotal_s1_r1_e1",
                         "bfne_b_scrdTotal_s1_r1_e1", "epepq15_scrdTotal_s1_r1_e1", "phq8_scrdTotal_s1_r1_e1",
                         "stai5_scrdS_s1_r1_e1", "stai5_scrdS_s1_r1_e2", "stai5_scrdS_diff", "postTask_d_s1_r1_e1")]

redcapDat_sfe <- redcapDat_sfe[c("record_id", "scaared_b_scrdSoc_s1_r1_e1", "phq8_scrdTotal_s1_r1_e1", "scaared_b_scrdTotal_s1_r1_e1", "bfne_b_scrdTotal_s1_r1_e1", "stai5_scrdS_s1_r1_e1")]

# adding new columns to the "main_df" dataframe from redcapDat
for (rr in 1:nrow(main_df)){
  temp_id <- main_df$participant_id[rr]
  if (temp_id == 200005){ # Participant 200005 has completed sfe surveys (ID: 160077) instead of mfe_b surveys. So, I load this data from sfe redcap dataframe.
    # Participant 200005 has completed sfe surveys (ID: 160077) instead of mfe_b surveys. So, I only add their pre-task surveys.
    temp_id <- 160077
    tempDat <- filter(redcapDat_sfe, record_id == temp_id)
  } else {
    tempDat <- filter(redcapDat, record_id == temp_id)
  }
  if (nrow(tempDat) == 1){
    if (temp_id == 160077){
      main_df$scaared_b_scrdSoc_s1_r1_e1[rr] <- tempDat$scaared_b_scrdSoc_s1_r1_e1
      main_df$scaared_b_scrdTotal_s1_r1_e1[rr] <- tempDat$scaared_b_scrdTotal_s1_r1_e1
      main_df$bfne_b_scrdTotal_s1_r1_e1[rr] <- tempDat$bfne_b_scrdTotal_s1_r1_e1
      main_df$phq8_scrdTotal_s1_r1_e1[rr] <- tempDat$phq8_scrdTotal_s1_r1_e1
      main_df$stai5_scrdS_s1_r1_e1[rr] <- tempDat$stai5_scrdS_s1_r1_e1
      main_df$epepq15_scrdTotal_s1_r1_e1[rr] <- NA
      main_df$stai5_scrdS_s1_r1_e2[rr] <- NA
      main_df$stai5_scrdS_diff[rr] <- NA
      main_df$postTask_d_s1_r1_e1[rr] <- NA
    } else {
      main_df$scaared_b_scrdSoc_s1_r1_e1[rr] <- tempDat$scaared_b_scrdSoc_s1_r1_e1
      main_df$scaared_b_scrdTotal_s1_r1_e1[rr] <- tempDat$scaared_b_scrdTotal_s1_r1_e1
      main_df$bfne_b_scrdTotal_s1_r1_e1[rr] <- tempDat$bfne_b_scrdTotal_s1_r1_e1
      main_df$epepq15_scrdTotal_s1_r1_e1[rr] <- tempDat$epepq15_scrdTotal_s1_r1_e1
      main_df$phq8_scrdTotal_s1_r1_e1[rr] <- tempDat$phq8_scrdTotal_s1_r1_e1
      main_df$stai5_scrdS_s1_r1_e1[rr] <- tempDat$stai5_scrdS_s1_r1_e1
      main_df$stai5_scrdS_s1_r1_e2[rr] <- tempDat$stai5_scrdS_s1_r1_e2
      main_df$stai5_scrdS_diff[rr] <- tempDat$stai5_scrdS_diff
      main_df$postTask_d_s1_r1_e1[rr] <- tempDat$postTask_d_s1_r1_e1
    }
  } else if (nrow(tempDat) == 0){
    main_df$scaared_b_scrdSoc_s1_r1_e1[rr] <- NA
    main_df$scaared_b_scrdTotal_s1_r1_e1[rr] <- NA
    main_df$bfne_b_scrdTotal_s1_r1_e1[rr] <- NA
    main_df$epepq15_scrdTotal_s1_r1_e1[rr] <- NA
    main_df$phq8_scrdTotal_s1_r1_e1[rr] <- NA
    main_df$stai5_scrdS_s1_r1_e1[rr] <- NA
    main_df$stai5_scrdS_s1_r1_e2[rr] <- NA
    main_df$stai5_scrdS_diff[rr] <- NA
    main_df$postTask_d_s1_r1_e1[rr] <- NA
  }
}

### Loading EEG MFC Theta power data
theta_power_mfc <- read.csv(file = "/Users/kihossei/Documents/GitHub/memory-for-error-dataset/derivatives/preprocessed/eeg/TF_outputs/csv_for_stats/theta_power_mfc.csv")

# Keeping the columns that we need!
theta_power_mfc <- theta_power_mfc[c("id", "incong_error_theta_power", "incong_correct_theta_power", "difference_score")]

# adding new columns to the "main_df" dataframe from eeg_erp
for (rr in 1:nrow(main_df)){
  temp_id <- main_df$participant_id[rr]
  tempDat <- filter(theta_power_mfc, id == temp_id)
  if (nrow(tempDat) == 1 && temp_id != 200049){ # only behavioral for sub 49 will be included.
    main_df$theta_power_mfc_error[rr] <- tempDat$incong_error_theta_power
    main_df$theta_power_mfc_correct[rr] <- tempDat$incong_correct_theta_power
    main_df$theta_power_mfc_difference_score[rr] <- tempDat$difference_score
  } else if (nrow(tempDat) == 0 || temp_id == 200049){
    main_df$theta_power_mfc_error[rr] <- NA
    main_df$theta_power_mfc_correct[rr] <- NA
    main_df$theta_power_mfc_difference_score[rr] <- NA
  }
}

### Loading EEG MFC Theta ITPS data
theta_itps_mfc <- read.csv(file = "/Users/kihossei/Documents/GitHub/memory-for-error-dataset/derivatives/preprocessed/eeg/TF_outputs/csv_for_stats/ITPS_mfc.csv")

# Keeping the columns that we need!
theta_itps_mfc <- theta_itps_mfc[c("id", "incong_error_ITPS", "incong_correct_ITPS", "difference_score")]

# adding new columns to the "main_df" dataframe from eeg_erp
for (rr in 1:nrow(main_df)){
  temp_id <- main_df$participant_id[rr]
  tempDat <- filter(theta_itps_mfc, id == temp_id)
  if (nrow(tempDat) == 1 && temp_id != 200049){
    main_df$theta_itps_mfc_error[rr] <- tempDat$incong_error_ITPS
    main_df$theta_itps_mfc_correct[rr] <- tempDat$incong_correct_ITPS
    main_df$theta_itps_mfc_difference_score[rr] <- tempDat$difference_score
  } else if (nrow(tempDat) == 0 || temp_id == 200049){
    main_df$theta_itps_mfc_error[rr] <- NA
    main_df$theta_itps_mfc_correct[rr] <- NA
    main_df$theta_itps_mfc_difference_score[rr] <- NA
  }
}

### Loading EEG MFC Theta ITPS data
theta_wpli_lat_occipital <- read.csv(file = "/Users/kihossei/Documents/GitHub/memory-for-error-dataset/derivatives/preprocessed/eeg/TF_outputs/csv_for_stats/wPLI_lat_occipital.csv")

# Keeping the columns that we need!
theta_wpli_lat_occipital <- theta_wpli_lat_occipital[c("id", "incong_error_wPLI", "incong_correct_wPLI", "difference_score")]

# adding new columns to the "main_df" dataframe from eeg_erp
for (rr in 1:nrow(main_df)){
  temp_id <- main_df$participant_id[rr]
  tempDat <- filter(theta_wpli_lat_occipital, id == temp_id)
  if (nrow(tempDat) == 1 && temp_id != 200049){
    main_df$theta_wpli_lat_occipital_error[rr] <- tempDat$incong_error_wPLI
    main_df$theta_wpli_lat_occipital_correct[rr] <- tempDat$incong_correct_wPLI
    main_df$theta_wpli_lat_occipital_difference_score[rr] <- tempDat$difference_score
  } else if (nrow(tempDat) == 0 || temp_id == 200049){
    main_df$theta_wpli_lat_occipital_error[rr] <- NA
    main_df$theta_wpli_lat_occipital_correct[rr] <- NA
    main_df$theta_wpli_lat_occipital_difference_score[rr] <- NA
  }
}
# adding a new column that categorizes each participant into either low or high SA group according to Median.
# for this data, median is 6. So, I will split dataframe into 2 according to 6.

main_df <- main_df %>%
  add_column(SA_group = NA)

main_df <- main_df[order(main_df$scaared_b_scrdSoc_s1_r1_e1),] # order dataframe rows based on social anxiety scaared scores

for (trdr in 1:nrow(main_df)){
  if (!is.na(main_df$scaared_b_scrdSoc_s1_r1_e1[trdr])){
    if (trdr <= round(nrow(main_df)/2)){
      main_df$SA_group[trdr] <- 'low'
    } else if (trdr >= round((nrow(main_df)/2))+1){
      main_df$SA_group[trdr] <- 'high'
    }
  } else if (is.na(main_df$scaared_b_scrdSoc_s1_r1_e1[trdr])){
    main_df$SA_group[trdr] <- NA
  }
}


####################
# Save the dataset
#write the extracted and computed summary scores to disk
write.csv(main_df, paste(output_path, proc_fileName, sep = "/", collapse = NULL), row.names=FALSE)
##################
### Kianoosh has checked up to this point and no error.

